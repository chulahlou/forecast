{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9995a06d-b907-4c91-9e1a-7b2250e8d272",
   "metadata": {},
   "source": [
    "# Probabilistic Forecasting\n",
    "\n",
    "This notebook demonstrates the usage of autoregressive deep neural prediction models on synethic time-series sales data. The data generator produces a rich synthetic dataset with many-to-many relationships such as product-location pairs. \n",
    "\n",
    "The demo code requires the following requirements to be met: \n",
    "\n",
    "```\n",
    "gluonts>=0.8.1\n",
    "pandas>=1.3\n",
    "torch>=1.9\n",
    "numpy>=1.21.4\n",
    "```\n",
    "\n",
    "### Example Usage\n",
    "\n",
    "To begin, we import a few libraries that will help in generating some synthetic sales data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e417a871-be71-450d-9417-0ac62d2f1ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ca8a1f-93c3-4c53-9bab-ff461aab8f76",
   "metadata": {},
   "source": [
    "First, we'll declare an Enum object called `SalesDataModel` to represent the different types of sales data trends we can generate: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511fc407-916c-470a-9417-e615ec52bd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "class SalesDataModel(Enum):\n",
    "    RANDOM = 0\n",
    "    SINUSOID = 1\n",
    "    SPARSE = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ede53fc-860a-41f3-a609-aa5dcd394f94",
   "metadata": {},
   "source": [
    "Next, we define the `SalesDataGenerator` class that takes a couple of arguments that define the characteristics of the data to be generated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff46ad7a-b1e5-4bf1-b8e7-06c50981017b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Mapping\n",
    "\n",
    "class SalesDataGenerator:\n",
    "    def __init__(\n",
    "        self,\n",
    "        start_date: datetime,\n",
    "        num_days: int,\n",
    "        item_to_location: Mapping[str, List[str]],\n",
    "        item_to_class: Mapping[str, List[str]],\n",
    "        min_sales_per_day: int = 0,\n",
    "        max_sales_per_day: int = 1000,\n",
    "    ):\n",
    "        self.start_date = start_date\n",
    "        self.num_days = num_days\n",
    "        self.item_to_location = item_to_location\n",
    "        self.item_to_class = item_to_class\n",
    "        self.min_sales_per_day = min_sales_per_day\n",
    "        self.max_sales_per_day = max_sales_per_day\n",
    "\n",
    "    def synthesize(self, data_model: SalesDataModel = SalesDataModel.RANDOM):\n",
    "        df = pd.DataFrame(columns=[\"Date\", \"Item\", \"Location\", \"Sales\"])\n",
    "        for item, locations in self.item_to_location.items():\n",
    "            for location in locations:\n",
    "                if data_model == SalesDataModel.RANDOM:\n",
    "                    sales = np.random.randint(\n",
    "                        self.min_sales_per_day, self.max_sales_per_day, self.num_days\n",
    "                    )\n",
    "                elif data_model == SalesDataModel.SINUSOID:\n",
    "                    alpha = 0.9\n",
    "                    gain = alpha * (self.max_sales_per_day - self.min_sales_per_day)\n",
    "                    periods = np.random.randint(1, 6, 1)\n",
    "                    nn = np.linspace(0, int(periods), self.num_days)\n",
    "                    base_sales = (\n",
    "                        gain * np.abs(np.cos(np.pi * nn))\n",
    "                        + alpha * self.min_sales_per_day\n",
    "                    )\n",
    "                    noise = np.random.randint(\n",
    "                        (1 - alpha) * self.min_sales_per_day,\n",
    "                        (1 - alpha) * self.max_sales_per_day,\n",
    "                        self.num_days,\n",
    "                    )\n",
    "                    sales = base_sales + noise\n",
    "                    sales = sales.astype(int).tolist()\n",
    "                elif data_model == SalesDataModel.SPARSE:\n",
    "                    thresh = 0.1\n",
    "                    sales = [\n",
    "                        0\n",
    "                        if np.random.rand() > thresh\n",
    "                        else np.random.randint(\n",
    "                            self.min_sales_per_day, self.max_sales_per_day, 1\n",
    "                        )\n",
    "                        for _ in range(self.num_days)\n",
    "                    ]\n",
    "                _df = pd.DataFrame(\n",
    "                    {\n",
    "                        \"Date\": [\n",
    "                            self.start_date + timedelta(days=diff)\n",
    "                            for diff in range(self.num_days)\n",
    "                        ],\n",
    "                        \"Item\": [item] * self.num_days,\n",
    "                        \"Location\": [location] * self.num_days,\n",
    "                        \"Sales\": sales,\n",
    "                        \"Item Class\": [self.item_to_class[item]] * self.num_days,\n",
    "                    }\n",
    "                )\n",
    "                df = pd.concat([df, _df], ignore_index=True)\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44889b9c-f778-4a82-a8ae-d8eea76f601c",
   "metadata": {},
   "source": [
    "For example, let's set the parameters to generate sales data over 5 years (`num_days`) backwards from today. We'll include 20 items, with each item being sold at 5 locations. Each item belongs to a item class randomly selected from 3 classes (this will be a static covariate when we get to the modeling section). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3005709-e2e5-4c79-8b76-c1af031f869a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_days = 365 * 5\n",
    "items = [f\"Item {idx}\" for idx in range(20)]\n",
    "locations = [f\"Location {idx}\" for idx in range(5)]\n",
    "item_to_class = {\n",
    "    item: random.choice([\"Item Class A\", \"Item Class B\", \"Item Class C\"])\n",
    "    for item in items\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0afc3c6f-559f-4c4b-980b-26816e1148e8",
   "metadata": {},
   "source": [
    "Next, we generate the data. For this example, we'll generate sinusoidal data (which also incorporates noise). Note that the `min_sales_per_day` argument is negative, meaning that the net sales for the day could be items returned. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6afc8c8-2d32-41be-bacf-87dd83f87b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = SalesDataGenerator(\n",
    "    start_date=datetime.today().date() - timedelta(days=num_days),\n",
    "    num_days=num_days,\n",
    "    item_to_location={item: locations for item in items},\n",
    "    item_to_class=item_to_class,\n",
    "    max_sales_per_day=1000,\n",
    "    min_sales_per_day=-100,\n",
    ")\n",
    "data = generator.synthesize(data_model=SalesDataModel.SINUSOID)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac762ecc-544a-4fe1-b407-c26ac06fda78",
   "metadata": {},
   "source": [
    "The object generated is a pandas dataframe, so we can peak at the data as follows: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed32507-5d15-42bb-bf28-1f8d53ba7b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214095b2-a7ff-4886-9a55-5762e56802b0",
   "metadata": {},
   "source": [
    "With the dataset defined, we next need to preprocess it compatable with the model training object. To do this, we create a list of dictionaries with specific keys. Note that we are scaling the data in this preprocessing step to be compatable with the statistical structure of the model we define later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6ad3a2-ef45-448a-8c0d-c01ddd72104d",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = data.Date.min()\n",
    "end = data.Date.max()\n",
    "ensemble = []\n",
    "dates = list(pd.date_range(start, end))\n",
    "for item in set(data.Item): \n",
    "    for location in set(data.Location):\n",
    "        sales = data[(data.Item == item) & (data.Location == location)].Sales.values\n",
    "        if len(sales): \n",
    "            scale = max(abs(sales))\n",
    "            ensemble.append(\n",
    "                {\n",
    "                    \"class\": item_to_class[item],\n",
    "                    \"item\": item,\n",
    "                    \"sales\": sales,\n",
    "                    \"start\": start,\n",
    "                    \"end\": end,\n",
    "                    \"total\": sum(sales),\n",
    "                    \"scale\": scale,\n",
    "                    \"location\": location,\n",
    "                    \"sales_scaled\": sales / scale,\n",
    "                }\n",
    "            )\n",
    "ensemble = sorted(ensemble, key=lambda k: k[\"total\"], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48855e97-e5c3-4036-b6ed-a02302056f0d",
   "metadata": {},
   "source": [
    "The probabalistic neural model we are using requires specific objects to be created to capture the (1) time series data and (2) covariates. The following code snippet produces these: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971789aa-74d2-4649-b3a8-677028c36ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gluonts.dataset.common import load_datasets, ListDataset\n",
    "from gluonts.dataset.field_names import FieldName\n",
    "\n",
    "quantity_length = len(ensemble[0][\"sales\"])\n",
    "prediction_length = 30\n",
    "context_length = 6 * prediction_length\n",
    "\n",
    "static_cats = [(e[\"class\"].strip(), e[\"item\"].strip(), e[\"location\"].strip()) for e in ensemble]\n",
    "\n",
    "item_to_index = {item: idx for idx, item in enumerate(set([sc[0] for sc in static_cats]))}\n",
    "item_class_to_index = {item: idx for idx, item in enumerate(set([sc[1] for sc in static_cats]))}\n",
    "location_to_index = {loc: idx for idx, loc in enumerate(set([sc[2] for sc in static_cats]))}\n",
    "\n",
    "test_target_values = np.array([np.array(e[\"sales_scaled\"]) for e in ensemble])\n",
    "train_target_values = np.array([np.array(e[\"sales_scaled\"][:-prediction_length]) for e in ensemble])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41010b59-0b3f-49e1-8de2-6c5990672f95",
   "metadata": {},
   "source": [
    "Next, we build the training and testing data splits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c68b8b2-a0db-49b2-93b1-a4a48c8c5d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = ListDataset([\n",
    "    {\n",
    "        FieldName.TARGET: target,\n",
    "        FieldName.START: start,\n",
    "        FieldName.FEAT_STATIC_CAT: [\n",
    "            item_to_index[sc_item], \n",
    "            item_class_to_index[sc_item_class], \n",
    "            location_to_index[sc_location]\n",
    "        ],\n",
    "    }\n",
    "    for target, (sc_item, sc_item_class, sc_location) in zip(train_target_values, static_cats)\n",
    "], freq=\"D\")\n",
    "\n",
    "test_ds = ListDataset([\n",
    "    {\n",
    "        FieldName.TARGET: target,\n",
    "        FieldName.START: start,\n",
    "        FieldName.FEAT_STATIC_CAT: [\n",
    "            item_to_index[sc_item], \n",
    "            item_class_to_index[sc_item_class], \n",
    "            location_to_index[sc_location]\n",
    "        ],\n",
    "    }\n",
    "    for target, (sc_item, sc_item_class, sc_location) in zip(test_target_values, static_cats)\n",
    "], freq=\"D\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5a4c2e-ddfe-4e44-bf6e-ed876fb30c1f",
   "metadata": {},
   "source": [
    "The model architecture is defined below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f6a85d-d980-4a37-93b4-17d6ff4db175",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gluonts.model.deepar import DeepAREstimator\n",
    "from gluonts.mx.distribution.gaussian import GaussianOutput\n",
    "from gluonts.mx.trainer import Trainer\n",
    "\n",
    "estimator = DeepAREstimator(\n",
    "    prediction_length=prediction_length,\n",
    "    context_length=context_length,\n",
    "    freq=\"D\",\n",
    "    num_cells = 2,\n",
    "    distr_output = GaussianOutput(),\n",
    "    use_feat_dynamic_real=False,\n",
    "    use_feat_static_cat=True,\n",
    "    cardinality=[len(item_to_index), len(item_class_to_index), len(location_to_index)],\n",
    "    embedding_dimension=[32,64, 32],\n",
    "    trainer=Trainer(\n",
    "        learning_rate=1e-2,\n",
    "        epochs=20,\n",
    "        num_batches_per_epoch=10,\n",
    "        clip_gradient=1.0\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae6c35c-523c-48a4-bde5-391784083ed7",
   "metadata": {},
   "source": [
    "Commence model training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b179ba81-ee0e-4925-b0be-84d1bb612e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = estimator.train(train_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a259523b-1fa5-49c7-824e-746083f60594",
   "metadata": {},
   "source": [
    "With the model trained, we now look to evaluate the performance on the test dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7e6d5f-1544-463d-b3df-1bcde5b03dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gluonts.evaluation.backtest import make_evaluation_predictions\n",
    "\n",
    "forecast_it, ts_it = make_evaluation_predictions(dataset=test_ds, predictor=predictor, num_samples=250)\n",
    "forecasts = [f for f in forecast_it]\n",
    "tss = list(ts_it)\n",
    "\n",
    "from gluonts.evaluation import make_evaluation_predictions, Evaluator\n",
    "\n",
    "evaluator = Evaluator(quantiles=[0.1, 0.5, 0.9])\n",
    "metrics, _ = evaluator(iter(tss), iter(forecasts), num_series=len(test_ds))\n",
    "metrics = pd.DataFrame.from_records(metrics, index=[\"DeepAR\"]).transpose()\n",
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ec3b82-b088-4591-b6a8-3333404ca7d4",
   "metadata": {},
   "source": [
    "To help visualize the results, we next plot a few of the probabalistic predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a480ae-6850-4832-abea-14159fdac2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import islice\n",
    "plt.figure(figsize=(20, 15))\n",
    "plt.rcParams.update({'font.size': 15})\n",
    "\n",
    "for idx, (f, ts) in islice(enumerate(zip(forecasts, tss)), 9):\n",
    "    ax = plt.subplot(3, 3, idx+1)\n",
    "\n",
    "    plt.plot(ensemble[idx]['scale'] * ts[-15 * prediction_length:], label=\"target\")\n",
    "    f2 = copy.deepcopy(f)\n",
    "    f2.samples *= ensemble[idx]['scale']\n",
    "    f2.plot()\n",
    "    plt.xticks(rotation=60)\n",
    "    plt.title(f\"{ensemble[idx]['item']}\")\n",
    "\n",
    "plt.gcf().tight_layout()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d155f558-d25c-451b-a3d9-73fd0347e4ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "forecast-ml",
   "language": "python",
   "name": "forecast-ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
